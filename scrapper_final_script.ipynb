{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\alebe\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url =' https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "dict_keys([]) dict_keys(['Table_1', 'Table_2', 'Table_3'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Table_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-e7c40e7603cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_titles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_tables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_titles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_tables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinaldf_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-e7c40e7603cf>\u001b[0m in \u001b[0;36mname_tables\u001b[1;34m(titles, tables)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mname_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-e7c40e7603cf>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mname_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Table_1'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "def define_title(soup, tag, class_type, class_value):\n",
    "    scrambled_titles_list = []\n",
    "    scrambled_titles = soup.find_all(tag, {tag : class_value})\n",
    "\n",
    "    for scrambled_title in scrambled_titles:\n",
    "        scrambled_titles_list.append(scrambled_title)\n",
    "        \n",
    "    return scrambled_titles_list\n",
    "    \n",
    "td_class_titles = define_title(soup, 'table', \"style\", \"font-size:100%;\")\n",
    "\n",
    "def removed_tag_titles(soups=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "            \n",
    "    title_list = []\n",
    "\n",
    "    for soup in soups:\n",
    "        text_corpus = soup.find_all(tag) \n",
    "\n",
    "        if type(regex) == str:\n",
    "            regex_in = regex\n",
    "        elif type(regex) == list:\n",
    "            regex_in = '|'.join(regex)\n",
    "        else:\n",
    "            raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "        for node in text_corpus:\n",
    "            text = node.get_text()\n",
    "            text= re.sub(regex_in, '', text)\n",
    "            title_list.append(text)\n",
    "            \n",
    "    return title_list\n",
    "\n",
    "filtered_titles = removed_tag_titles(td_class_titles, 'b' , [\"\\[[^\\]]*\\]\", '\\n'])\n",
    "print(filtered_titles)\n",
    "\n",
    "\n",
    "def make_titles(arr):\n",
    "    final_titles = {}\n",
    "    j = 0\n",
    "    for i, val in enumerate(arr, 1):\n",
    "        if val != '\\n':\n",
    "            j += 1\n",
    "            final_titles[f'Table_{j}'] = val\n",
    "    return final_titles\n",
    "\n",
    "final_titles = make_titles(filtered_titles)\n",
    "\n",
    "\n",
    "\n",
    "def locate_tables(soup=soup):\n",
    "    tbody_class_tables_dict = {}\n",
    "    wikitable_sortable_class = soup.find_all(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "    for i, wiki_table in enumerate(wikitable_sortable_class):\n",
    "        tbody_class_tables_dict[f'Table_{i + 1}'] = wikitable_sortable_class[i]\n",
    "            \n",
    "    return tbody_class_tables_dict\n",
    "\n",
    "tbody_class_tables_dict = locate_tables(soup)\n",
    "\n",
    "def removed_tag_tables(soup=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "    text_corpus = soup.find_all(tag) \n",
    "    final_text = []\n",
    "    \n",
    "    if type(regex) == str:\n",
    "        regex_in = regex\n",
    "    elif type(regex) == list:\n",
    "        regex_in = '|'.join(regex)\n",
    "    else:\n",
    "        raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "    for node in text_corpus:\n",
    "        text = node.get_text()\n",
    "        text= re.sub(regex_in, '', text)\n",
    "        final_text.append(text)\n",
    "    return final_text\n",
    "\n",
    "def filter_tables(tables_dict):        \n",
    "    filtered_table_dict = {}\n",
    "    for key, val in tables_dict.items():\n",
    "\n",
    "        tbody_class_table_clean = removed_tag_tables(val, 'tr' , [\"\\[[^\\]]*\\]\", '\\xa0', ',', '\\n$', '^\\n'])\n",
    "        filtered_table_dict[key] = tbody_class_table_clean\n",
    "        \n",
    "    return filtered_table_dict\n",
    "\n",
    "filter_tables_dict = filter_tables(tbody_class_tables_dict)\n",
    "\n",
    "\n",
    "def fill_nulls(tables_dict):\n",
    "    import numpy as np\n",
    "    filled_filter_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filled_nulls_table = []\n",
    "        for i, row in enumerate(table):\n",
    "            if '\\n' in row[0:1]:\n",
    "                row_filled = 'NaN' + row           \n",
    "                filled_nulls_table.append(row_filled)\n",
    "            else:\n",
    "                row_filled = row\n",
    "                filled_nulls_table.append(row_filled)\n",
    "        filled_filter_tables_dict[key] = filled_nulls_table\n",
    "    return filled_filter_tables_dict\n",
    "    \n",
    "filtered_tables_rows = fill_nulls(filter_tables_dict)\n",
    "\n",
    "\n",
    "def make_tables(tables_dict):\n",
    "    import re   \n",
    "    final_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filtered_tables_rows_dict = {}\n",
    "        for i, val in enumerate(table, 1):\n",
    "            row = re.split(r\"\\n\", val)\n",
    "            while \"\" in row:\n",
    "                row.remove(\"\")\n",
    "            filtered_tables_rows_dict[f'Row_{i}'] = row\n",
    "\n",
    "        final_tables_dict[key] = filtered_tables_rows_dict\n",
    "    return final_tables_dict\n",
    "    \n",
    "final_rows = make_tables(filtered_tables_rows)\n",
    "\n",
    "def make_dataframe(tables_dict):\n",
    "    final_tables = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        final_tables[key] = pd.DataFrame.from_dict(table).transpose()\n",
    "        final_tables[key] = final_tables.get(key).reset_index(drop = True)\n",
    "        final_tables[key].columns = final_tables[key].iloc[0]\n",
    "        final_tables[key] = final_tables[key][1:]\n",
    "    return final_tables\n",
    "\n",
    "final_tables = make_dataframe(final_rows)\n",
    "\n",
    "def name_tables(titles, tables):\n",
    "\n",
    "    final_df = {titles[key] : value for key, value in tables.items()} \n",
    "    return final_df\n",
    "\n",
    "print(final_titles.keys(), final_tables.keys())\n",
    "\n",
    "final_df = name_tables(final_titles, final_tables)\n",
    "\n",
    "def finaldf_to_csv(final_df):\n",
    "    for i in final_df: \n",
    "        final_df[i].to_csv('GDP(Nominal)'+str(i)+'.csv')\n",
    "        print('CSV files saved!')\n",
    "finaldf_to_csv(final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
