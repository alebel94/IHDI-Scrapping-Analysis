{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\alebe\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org/wiki/World_Happiness_Report'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table_4 could not be added, the length of the rows are not the same\n",
      "Table_5 could not be added, the length of the rows are not the same\n",
      "Table_1 was saved in CSV format!\n",
      "Table_2 was saved in CSV format!\n",
      "Table_3 was saved in CSV format!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "def define_title(soup, tag, class_type, class_value):\n",
    "    scrambled_titles_list = []\n",
    "    scrambled_titles = soup.find_all(tag, {class_type : class_value})\n",
    "\n",
    "    for scrambled_title in scrambled_titles:\n",
    "        scrambled_titles_list.append(scrambled_title)\n",
    "        \n",
    "    return scrambled_titles_list\n",
    "    \n",
    "td_class_titles = define_title(soup, 'span', \"class\", \"mw-headline\")\n",
    "\n",
    "def removed_tag_titles(soups=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "\n",
    "    title_list = []\n",
    "\n",
    "    for soup in soups:\n",
    "        text_corpus = soup.find_all(tag) \n",
    "        if tag == '':\n",
    "            text_corpus = soups\n",
    "        if type(regex) == str:\n",
    "            regex_in = regex\n",
    "        elif type(regex) == list:\n",
    "            regex_in = '|'.join(regex)\n",
    "        else:\n",
    "            raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "        for node in text_corpus:\n",
    "            text = node.get_text()\n",
    "            text= re.sub(regex_in, '', text)\n",
    "            title_list.append(text)\n",
    "            \n",
    "    return title_list\n",
    "\n",
    "filtered_titles = removed_tag_titles(td_class_titles, '' , [\"\\[[^\\]]*\\]\", '\\n'])\n",
    "\n",
    "\n",
    "def make_titles(arr):\n",
    "    final_titles = {}\n",
    "    j = 0\n",
    "    for i, val in enumerate(arr, 1):\n",
    "        if val != '\\n':\n",
    "            j += 1\n",
    "            final_titles[f'Table_{j}'] = val\n",
    "    return final_titles\n",
    "\n",
    "final_titles = make_titles(filtered_titles)\n",
    "\n",
    "\n",
    "\n",
    "def locate_tables(soup=soup):\n",
    "    tbody_class_tables_dict = {}\n",
    "    wikitable_sortable_class = soup.find_all(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "    for i, wiki_table in enumerate(wikitable_sortable_class):\n",
    "        tbody_class_tables_dict[f'Table_{i + 1}'] = wikitable_sortable_class[i]\n",
    "            \n",
    "    return tbody_class_tables_dict\n",
    "\n",
    "tbody_class_tables_dict = locate_tables(soup)\n",
    "\n",
    "def removed_tag_tables(soup=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "    text_corpus = soup.find_all(tag) \n",
    "    final_text = []\n",
    "    \n",
    "    if type(regex) == str:\n",
    "        regex_in = regex\n",
    "    elif type(regex) == list:\n",
    "        regex_in = '|'.join(regex)\n",
    "    else:\n",
    "        raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "    for node in text_corpus:\n",
    "        text = node.get_text()\n",
    "        text= re.sub(regex_in, '', text)\n",
    "        final_text.append(text)\n",
    "    return final_text\n",
    "\n",
    "def filter_tables(tables_dict):        \n",
    "    filtered_table_dict = {}\n",
    "    for key, val in tables_dict.items():\n",
    "\n",
    "        tbody_class_table_clean = removed_tag_tables(val, 'tr' , [\"\\[[^\\]]*\\]\", '\\xa0', ',', '\\n$', '^\\n'])\n",
    "        filtered_table_dict[key] = tbody_class_table_clean\n",
    "        \n",
    "    return filtered_table_dict\n",
    "\n",
    "filter_tables_dict = filter_tables(tbody_class_tables_dict)\n",
    "\n",
    "\n",
    "def fill_nulls(tables_dict):\n",
    "    import numpy as np\n",
    "    filled_filter_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filled_nulls_table = []\n",
    "        for i, row in enumerate(table):\n",
    "            if '\\n' in row[0:1]:\n",
    "                row_filled = 'NaN' + row           \n",
    "                filled_nulls_table.append(row_filled)\n",
    "            else:\n",
    "                row_filled = row\n",
    "                filled_nulls_table.append(row_filled)\n",
    "        filled_filter_tables_dict[key] = filled_nulls_table\n",
    "    return filled_filter_tables_dict\n",
    "    \n",
    "filtered_tables_rows = fill_nulls(filter_tables_dict)\n",
    "\n",
    "def make_tables(tables_dict):\n",
    "    import re   \n",
    "    final_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filtered_tables_rows_dict = {}\n",
    "        for i, val in enumerate(table, 1):\n",
    "            row = re.split(r\"\\n\", val)\n",
    "            while \"\" in row:\n",
    "                row.remove(\"\")\n",
    "            filtered_tables_rows_dict[f'Row_{i}'] = row\n",
    "\n",
    "        final_tables_dict[key] = filtered_tables_rows_dict\n",
    "    return final_tables_dict\n",
    "    \n",
    "final_rows = make_tables(filtered_tables_rows)\n",
    "\n",
    "#tables_dict = {key: tables{key: rows[values]}}\n",
    "\n",
    "def make_dataframe(tables_dict):\n",
    "    tables_to_df = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        same = True\n",
    "        \n",
    "        for i in range(len(table.values()) - 1):\n",
    "            length = len(list(table.values())[i])\n",
    "            if len(list(table.values())[i+1]) == length:\n",
    "#                 print(f'good: {list(table.values())[i], list(table.values())[i+1]}')\n",
    "                pass\n",
    "            else:\n",
    "#                 print(f'bad: {list(table.values())[i], list(table.values())[i+1]}')\n",
    "                same = False\n",
    "                        \n",
    "        if same == True:\n",
    "            tables_to_df[key] = table\n",
    "        else:\n",
    "            print(f'{key} could not be added, the length of the rows are not the same')\n",
    "            same = True\n",
    "        \n",
    "    final_tables = {}\n",
    "\n",
    "    \n",
    "    for key, table in tables_to_df.items():\n",
    "        final_tables[key] = pd.DataFrame.from_dict(table).transpose()\n",
    "        final_tables[key] = final_tables.get(key).reset_index(drop = True)\n",
    "        final_tables[key].columns = final_tables[key].iloc[0]\n",
    "        final_tables[key] = final_tables[key][1:]\n",
    "    return final_tables\n",
    "\n",
    "final_tables = make_dataframe(final_rows)\n",
    "\n",
    "def name_tables(titles, tables):\n",
    "\n",
    "    final_df = {titles[key] : value for key, value in tables.items()} \n",
    "    return final_df\n",
    "\n",
    "\n",
    "final_df = name_tables(final_titles, final_tables)\n",
    "\n",
    "def finaldf_to_csv(final_df):\n",
    "    for i, table in enumerate(final_df): \n",
    "        final_df[table].to_csv('GDP(Nominal)'+str(table)+'.csv')\n",
    "        print(f'Table_{i + 1} was saved in CSV format!')\n",
    "finaldf_to_csv(final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
