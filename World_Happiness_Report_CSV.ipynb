{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\alebe\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alebe\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org/wiki/World_Happiness_Report'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trable Table_4 could not be added, would you like to add the table with missing values?(y/n) \n",
      "n\n",
      "Table_4 was not be added, the length of the rows are not the same\n",
      "Trable Table_5 could not be added, would you like to add the table with missing values?(y/n) \n",
      "n\n",
      "Table_5 was not be added, the length of the rows are not the same\n",
      "Table_1 was saved in CSV format!\n",
      "Table_2 was saved in CSV format!\n",
      "Table_3 was saved in CSV format!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "def define_title(soup, tag, class_type, class_value):\n",
    "    scrambled_titles_list = []\n",
    "    scrambled_titles = soup.find_all(tag, {class_type : class_value})\n",
    "\n",
    "    for scrambled_title in scrambled_titles:\n",
    "        scrambled_titles_list.append(scrambled_title)\n",
    "        \n",
    "    return scrambled_titles_list\n",
    "    \n",
    "td_class_titles = define_title(soup, 'span', \"class\", \"mw-headline\")\n",
    "\n",
    "def removed_tag_titles(soups=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "\n",
    "    title_list = []\n",
    "\n",
    "    for soup in soups:\n",
    "        text_corpus = soup.find_all(tag) \n",
    "        if tag == '':\n",
    "            text_corpus = soups\n",
    "        if type(regex) == str:\n",
    "            regex_in = regex\n",
    "        elif type(regex) == list:\n",
    "            regex_in = '|'.join(regex)\n",
    "        else:\n",
    "            raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "        for node in text_corpus:\n",
    "            text = node.get_text()\n",
    "            text= re.sub(regex_in, '', text)\n",
    "            title_list.append(text)\n",
    "            \n",
    "    return title_list\n",
    "\n",
    "filtered_titles = removed_tag_titles(td_class_titles, '' , [\"\\[[^\\]]*\\]\", '\\n'])\n",
    "\n",
    "\n",
    "def make_titles(arr):\n",
    "    final_titles = {}\n",
    "    j = 0\n",
    "    for i, val in enumerate(arr, 1):\n",
    "        if val != '\\n':\n",
    "            j += 1\n",
    "            final_titles[f'Table_{j}'] = val\n",
    "    return final_titles\n",
    "\n",
    "final_titles = make_titles(filtered_titles)\n",
    "\n",
    "\n",
    "\n",
    "def locate_tables(soup=soup):\n",
    "    tbody_class_tables_dict = {}\n",
    "    wikitable_sortable_class = soup.find_all(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "    for i, wiki_table in enumerate(wikitable_sortable_class):\n",
    "        tbody_class_tables_dict[f'Table_{i + 1}'] = wikitable_sortable_class[i]\n",
    "            \n",
    "    return tbody_class_tables_dict\n",
    "\n",
    "tbody_class_tables_dict = locate_tables(soup)\n",
    "\n",
    "def removed_tag_tables(soup=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "    text_corpus = soup.find_all(tag) \n",
    "    final_text = []\n",
    "    \n",
    "    if type(regex) == str:\n",
    "        regex_in = regex\n",
    "    elif type(regex) == list:\n",
    "        regex_in = '|'.join(regex)\n",
    "    else:\n",
    "        raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "    for node in text_corpus:\n",
    "        text = node.get_text()\n",
    "        text= re.sub(regex_in, '', text)\n",
    "        final_text.append(text)\n",
    "    return final_text\n",
    "\n",
    "def filter_tables(tables_dict):        \n",
    "    filtered_table_dict = {}\n",
    "    for key, val in tables_dict.items():\n",
    "\n",
    "        tbody_class_table_clean = removed_tag_tables(val, 'tr' , [\"\\[[^\\]]*\\]\", '\\xa0', ',', '\\n$', '^\\n'])\n",
    "        filtered_table_dict[key] = tbody_class_table_clean\n",
    "        \n",
    "    return filtered_table_dict\n",
    "\n",
    "filter_tables_dict = filter_tables(tbody_class_tables_dict)\n",
    "\n",
    "\n",
    "def fill_nulls(tables_dict):\n",
    "    import numpy as np\n",
    "    filled_filter_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filled_nulls_table = []\n",
    "        for i, row in enumerate(table):\n",
    "            if '\\n' in row[0:1]:\n",
    "                row_filled = 'NaN' + row           \n",
    "                filled_nulls_table.append(row_filled)\n",
    "            else:\n",
    "                row_filled = row\n",
    "                filled_nulls_table.append(row_filled)\n",
    "        filled_filter_tables_dict[key] = filled_nulls_table\n",
    "    return filled_filter_tables_dict\n",
    "    \n",
    "filtered_tables_rows = fill_nulls(filter_tables_dict)\n",
    "\n",
    "def make_tables(tables_dict):\n",
    "    import re   \n",
    "    final_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filtered_tables_rows_dict = {}\n",
    "        for i, val in enumerate(table, 1):\n",
    "            row = re.split(r\"\\n\", val)\n",
    "            while \"\" in row:\n",
    "                row.remove(\"\")\n",
    "            filtered_tables_rows_dict[f'Row_{i}'] = row\n",
    "\n",
    "        final_tables_dict[key] = filtered_tables_rows_dict\n",
    "    return final_tables_dict\n",
    "    \n",
    "final_rows = make_tables(filtered_tables_rows)\n",
    "\n",
    "#tables_dict = {key: tables{key: rows[values]}}\n",
    "\n",
    "def make_dataframe(tables_dict):\n",
    "    tables_to_df = {}\n",
    "    removed_values = {}\n",
    "    for table_key, table in tables_dict.items():\n",
    "        same = True\n",
    "        len_avg = []\n",
    "        for i in range(len(table.values()) - 1):\n",
    "            length = len(list(table.values())[i])\n",
    "            len_avg.append(length)\n",
    "            if len(list(table.values())[i+1]) == length:\n",
    "#                 print(f'good: {list(table.values())[i], list(table.values())[i+1]}')\n",
    "                pass\n",
    "            else:\n",
    "#                 print(f'bad: {list(table.values())[i], list(table.values())[i+1]}')\n",
    "                same = False\n",
    "        if same == True:\n",
    "            tables_to_df[table_key] = table\n",
    "        else:\n",
    "            keep_rows = input(f'Trable {table_key} could not be added, would you like to add the table with missing values?(y/n) \\n')\n",
    "            while keep_rows.lower() not in [\"yes\", \"y\", \"no\", \"n\"]:\n",
    "                keep_rows = input(f'Sorry, \"{keep_rows}\" is not a valid input.\\nTable {table_key} could not be added, would you like to add the table with missing values?(y/n)')\n",
    "            if keep_rows.lower() == \"no\" or keep_rows.lower() == \"n\":\n",
    "                print(f'{table_key} was not be added, the length of the rows are not the same')\n",
    "                same = True\n",
    "            elif keep_rows.lower() == \"yes\" or keep_rows.lower() == \"y\":\n",
    "                mode = max(set(len_avg), key=len_avg.count)\n",
    "                \n",
    "                tables_to_df[table_key] = {}\n",
    "                removed_values = {}\n",
    "                for row_key, row in table.items():\n",
    "                    length = len(row)\n",
    "                    if len(row) == mode:\n",
    "                        tables_to_df[table_key][row_key] = row\n",
    "                    else:\n",
    "                        print(f'Popped key:{row_key} and pop value: {row}')\n",
    "                        removed_values[row_key] = row\n",
    "    final_tables = {}\n",
    "    for key, table in tables_to_df.items():\n",
    "        final_tables[key] = pd.DataFrame.from_dict(table).transpose()\n",
    "        final_tables[key] = final_tables.get(key).reset_index(drop = True)\n",
    "        final_tables[key].columns = final_tables[key].iloc[0]\n",
    "        final_tables[key] = final_tables[key][1:]\n",
    "    return final_tables, removed_values\n",
    "\n",
    "final_tables_tup = make_dataframe(final_rows)\n",
    "final_tables = final_tables_tup[0]\n",
    "removed_values = final_tables_tup[1]\n",
    "\n",
    "def name_tables(titles, tables):\n",
    "\n",
    "    final_df = {titles[key] : value for key, value in tables.items()} \n",
    "    return final_df\n",
    "\n",
    "\n",
    "final_df = name_tables(final_titles, final_tables)\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\alebe\\Documents\\coding-temple-jan2021\\capstone\\csv_tables'+'\\\\'\n",
    "doc_title= 'World_Happiness_Report'\n",
    "def finaldf_to_csv(final_df):\n",
    "    for i, table in enumerate(final_df): \n",
    "        final_df[table].to_csv(path+doc_title+str(table)+'.csv')\n",
    "        print(f'Table_{i + 1} was saved in CSV format!')\n",
    "finaldf_to_csv(final_df)\n",
    "\n",
    "def removeddf_to_csv(removed_values):\n",
    "    if removed_values != {}:    \n",
    "        with open(path+doc_title+'.txt', 'w') as output:\n",
    "            for key, row in removed_values.items():\n",
    "                output.write(f\"{str(key)}: {str(row)}\" + '\\n')\n",
    "        print(doc_title+' was saved in a TXT format!')\n",
    "\n",
    "removeddf_to_csv(removed_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table_4 could not be added, would you like to add the table with missing values?(y/n) \n",
      "y\n",
      "Popped key:Row_4 and pop value: ['3', 'Iceland', '7.504', ' 0.003', '1.481', '1.611', '0.834', '0.627', '0.476', '0.154', '2.323']\n",
      "Popped key:Row_6 and pop value: ['5', 'Finland', '7.469', ' 0.056', '1.444', '1.540', '0.809', '0.618', '0.245', '0.383', '2.430']\n",
      "Popped key:Row_9 and pop value: ['8', 'New Zealand', '7.314', ' 0.020', '1.406', '1.548', '0.817', '0.614', '0.500', '0.383', '2.046']\n",
      "Popped key:Row_10 and pop value: ['9', 'Australia', '7.284', ' 0.029', '1.484', '1.510', '0.844', '0.602', '0.478', '0.301', '2.065']\n",
      "Popped key:Row_11 and pop value: ['10', 'Sweden', '7.284', ' 0.007', '1.494', '1.478', '0.831', '0.613', '0.385', '0.384', '2.098']\n",
      "Popped key:Row_12 and pop value: ['11', 'Israel', '7.213', ' 0.054', '1.375', '1.376', '0.838', '0.406', '0.330', '0.085', '2.802']\n",
      "Popped key:Row_17 and pop value: ['16', 'Germany', '6.951', ' 0.043', '1.488', '1.473', '0.799', '0.563', '0.336', '0.277', '2.016']\n",
      "Popped key:Row_36 and pop value: ['35', ' 1', 'Qatar', '6.375', '1.871', '1.274', '0.710', '0.604', '0.330', '0.439', '1.145']\n",
      "Popped key:Row_39 and pop value: ['38', ' 5', 'Trinidad and Tobago', '6.168', '1.361', '1.380', '0.520', '0.519', '0.325', '0.009', '2.053']\n",
      "Popped key:Row_52 and pop value: ['50', ' 2', 'Belize', '5.956', '0.908', '1.081', '0.450', '0.548', '0.240', '0.097', '2.632']\n",
      "Popped key:Row_57 and pop value: ['55', 'Moldova', '5.838', ' 0.059', '0.729', '1.252', '0.589', '0.241', '0.209', '0.010', '2.808']\n",
      "Popped key:Row_72 and pop value: ['70', 'Paraguay', '5.493', ' 0.045', '0.933', '1.507', '0.579', '0.474', '0.224', '0.091', '1.685']\n",
      "Popped key:Row_112 and pop value: ['109', 'Albania', '4.644', ' 0.011', '0.996', '0.804', '0.731', '0.381', '0.201', '0.040', '1.490']\n",
      "Popped key:Row_113 and pop value: ['110', 'Bangladesh', '4.608', ' 0.035', '0.587', '0.735', '0.533', '0.478', '0.172', '0.124', '1.979']\n",
      "Popped key:Row_114 and pop value: ['111', ' 2', 'Namibia', '4.574', '0.964', '1.098', '0.339', '0.520', '0.077', '0.093', '1.482']\n",
      "Popped key:Row_124 and pop value: ['121', 'Armenia', '4.376', ' 0.016', '0.901', '1.007', '0.638', '0.198', '0.083', '0.027', '1.521']\n",
      "Popped key:Row_133 and pop value: ['130', ' 3', 'Sudan', '4.139', '0.660', '1.214', '0.291', '0.015', '0.182', '0.090', '1.687']\n",
      "Popped key:Row_157 and pop value: ['154', ' 3', 'Burundi', '2.905', '0.092', '0.630', '0.152', '0.060', '0.204', '0.084', '1.683']\n",
      "Table_5 could not be added, would you like to add the table with missing values?(y/n) \n",
      "y\n",
      "Popped key:Row_1 and pop value: ['Overall Rank', 'Country', 'Score', 'Change OverPrior Year', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Trust']\n",
      "Table_1 was saved in CSV format!\n",
      "Table_2 was saved in CSV format!\n",
      "Table_3 was saved in CSV format!\n",
      "Table_4 was saved in CSV format!\n",
      "Table_5 was saved in CSV format!\n",
      "World_Happiness_Report was saved in a TXT format! Table_4\n",
      "World_Happiness_Report was saved in a TXT format! Table_5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "\n",
    "### FILL OUT BELOW ###\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "doc_title= 'World_Happiness_Report'\n",
    "\n",
    "path_dir = r'C:\\Users\\alebe\\Documents\\coding-temple-jan2021\\capstone\\csv_tables'\n",
    "\n",
    "### FILL OUT ABOVE ###\n",
    "path_join = os.path.join(path_dir, doc_title)\n",
    "try: \n",
    "    path = os.makedirs(path_join)\n",
    "except OSError as error:  \n",
    "    print(error)   \n",
    "    \n",
    "\n",
    "    \n",
    "def locate_tables(soup=soup, tag=\"table\", class_type=\"class\", class_value=\"wikitable sortable\"):\n",
    "    tbody_class_tables_dict = {}\n",
    "    wikitable_sortable_class = soup.find_all(tag, {class_type : class_value})\n",
    "    for i, wiki_table in enumerate(wikitable_sortable_class):\n",
    "        tbody_class_tables_dict[f'Table_{i + 1}'] = wikitable_sortable_class[i]\n",
    "            \n",
    "    return tbody_class_tables_dict\n",
    "\n",
    "tbody_class_tables_dict = locate_tables(soup, \"table\", \"class\", \"wikitable\")\n",
    "#print(tbody_class_tables_dict)\n",
    "\n",
    "def removed_tag_tables(soup=soup, tag= '', regex = ''):\n",
    "    import re\n",
    "    text_corpus = soup.find_all(tag) \n",
    "    final_text = []\n",
    "    \n",
    "    if type(regex) == str:\n",
    "        regex_in = regex\n",
    "    elif type(regex) == list:\n",
    "        regex_in = '|'.join(regex)\n",
    "    else:\n",
    "        raise ValueError('Use only a regex string or list of regex strings')\n",
    "\n",
    "    for node in text_corpus:\n",
    "        text = node.get_text()\n",
    "        text= re.sub(regex_in, '', text)\n",
    "        final_text.append(text)\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def filter_tables(tables_dict):        \n",
    "    filtered_table_dict = {}\n",
    "    for key, val in tables_dict.items():\n",
    "        tbody_class_table_clean = removed_tag_tables(val, 'tr' , [\"\\[[^\\]]*\\]\", '\\xa0', ',', '\\n$', '^\\n', '\\xad', '\\u200b', '\\u0394'])\n",
    "        filtered_table_dict[key] = tbody_class_table_clean        \n",
    "\n",
    "\n",
    "    return filtered_table_dict\n",
    "\n",
    "filter_tables_dict = filter_tables(tbody_class_tables_dict)\n",
    "\n",
    "def fill_nulls(tables_dict):\n",
    "    #print(tables_dict)\n",
    "    \n",
    "    import numpy as np\n",
    "    filled_filter_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filled_nulls_table = []\n",
    "        for i, row in enumerate(table):\n",
    "#             while '\\n\\n\\n' in row:\n",
    "#                 row = row.replace('\\n\\n\\n', '\\nNaN\\n')\n",
    "#             while '\\n\\n' in row:\n",
    "#                 row = row.replace('\\n\\n', '\\nNaN\\n')\n",
    "#             if '\\n' in row[0:1]:\n",
    "#                 row_filled = 'NaN' + row     \n",
    "            if '\\n' in row[-2:]:\n",
    "                row_filled = row+'NaN' \n",
    "                filled_nulls_table.append(row_filled)\n",
    "            else:\n",
    "                row_filled = row\n",
    "                filled_nulls_table.append(row_filled)\n",
    "        filled_filter_tables_dict[key] = filled_nulls_table\n",
    "    return filled_filter_tables_dict\n",
    "    \n",
    "filtered_tables_rows = fill_nulls(filter_tables_dict)\n",
    "#print(filtered_tables_rows)\n",
    "\n",
    "def make_tables(tables_dict):\n",
    "    import re   \n",
    "    final_tables_dict = {}\n",
    "    for key, table in tables_dict.items():\n",
    "        filtered_tables_rows_dict = {}\n",
    "        for i, val in enumerate(table, 1):\n",
    "            row = re.split(r\"\\n\", val)\n",
    "            while \"\" in row:\n",
    "                row.remove(\"\")\n",
    "            filtered_tables_rows_dict[f'Row_{i}'] = row\n",
    "\n",
    "        final_tables_dict[key] = filtered_tables_rows_dict\n",
    "    return final_tables_dict\n",
    "    \n",
    "final_rows = make_tables(filtered_tables_rows)\n",
    "#print(final_rows)\n",
    "\n",
    "def make_dataframe(tables_dict):\n",
    "    tables_to_df = {}\n",
    "    removed_tables= {}\n",
    "    for table_key, table in tables_dict.items():\n",
    "        same = True\n",
    "        len_avg = []\n",
    "        for i in range(len(table.values()) - 1):\n",
    "            length = len(list(table.values())[i])\n",
    "            len_avg.append(length)\n",
    "            if len(list(table.values())[i+1]) == length:\n",
    "#                 print(f'good: {list(table.values())[i], list(table.values())[i+1]}')\n",
    "                pass\n",
    "            else:\n",
    "#                 print(f'bad: {list(table.values())[i], list(table.values())[i+1]}')\n",
    "                same = False\n",
    "        if same == True:\n",
    "            tables_to_df[table_key] = table\n",
    "        else:\n",
    "            keep_rows = input(f'{table_key} could not be added, would you like to add the table with missing values?(y/n) \\n')\n",
    "            while keep_rows.lower() not in [\"yes\", \"y\", \"no\", \"n\"]:\n",
    "                keep_rows = input(f'Sorry, \"{keep_rows}\" is not a valid input.\\nTable {table_key} could not be added, would you like to add the table with missing values?(y/n)')\n",
    "            if keep_rows.lower() == \"no\" or keep_rows.lower() == \"n\":\n",
    "                print(f'{table_key} was not be added, the length of the rows are not the same')\n",
    "                same = True\n",
    "            elif keep_rows.lower() == \"yes\" or keep_rows.lower() == \"y\":\n",
    "                mode = max(set(len_avg), key=len_avg.count)\n",
    "                \n",
    "                tables_to_df[table_key] = {}\n",
    "                removed_tables[table_key] = {}\n",
    "                for row_key, row in table.items():\n",
    "                    length = len(row)\n",
    "                    if len(row) == mode:\n",
    "                        tables_to_df[table_key][row_key] = row\n",
    "                    else:\n",
    "                        print(f'Popped key:{row_key} and pop value: {row}')\n",
    "                        removed_tables[table_key][row_key] = row\n",
    "    final_tables = {}\n",
    "    for key, table in tables_to_df.items():\n",
    "        final_tables[key] = pd.DataFrame.from_dict(table).transpose()\n",
    "        final_tables[key] = final_tables.get(key).reset_index(drop = True)\n",
    "        final_tables[key].columns = final_tables[key].iloc[0]\n",
    "        final_tables[key] = final_tables[key][1:]\n",
    "    return final_tables, removed_tables\n",
    "\n",
    "final_tables_tup = make_dataframe(final_rows)\n",
    "#print(final_tables_tup)\n",
    "final_tables = final_tables_tup[0]\n",
    "removed_tables = final_tables_tup[1]\n",
    "\n",
    "\n",
    "\n",
    "def finaldf_to_csv(final_df):\n",
    "    for i, table in enumerate(final_df): \n",
    "        final_df[table].to_csv(path_dir+'/'+doc_title+'/'+doc_title+'_'+str(table)+'.csv')\n",
    "        print(f'Table_{i + 1} was saved in CSV format!')\n",
    "finaldf_to_csv(final_tables)\n",
    "\n",
    "\n",
    "def removeddf_to_csv(removed_tables):\n",
    "    if removed_tables != {}: \n",
    "        for table in removed_tables:\n",
    "            with open(path_dir+'/'+doc_title+'/'+doc_title+'_'+str(table)+'.txt', 'w') as output:\n",
    "                for key, row in removed_tables[f'{table}'].items():\n",
    "                    output.write(f\"table:{str(table)}, row number:{str(key)}, row content: {str(row)}\" + '\\n')\n",
    "                print(doc_title+f' was saved in a TXT format! {table}')\n",
    "\n",
    "\n",
    "removeddf_to_csv(removed_tables)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
